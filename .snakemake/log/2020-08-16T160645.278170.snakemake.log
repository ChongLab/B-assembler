Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	longread_len
	1

[Sun Aug 16 16:06:45 2020]
rule longread_len:
    input: /data/user/huangf/Arginini/script-test/Only-long-read/snakemake/data/m150118_055507_00127_c100673182550000001823137502061570_s1_p0.bas.h5.fastq
    jobid: 0

[Sun Aug 16 16:06:45 2020]
Error in rule longread_len:
    jobid: 0
    shell:
        read_length_distribution.py /data/user/huangf/Arginini/script-test/Only-long-read/snakemake/data/m150118_055507_00127_c100673182550000001823137502061570_s1_p0.bas.h5.fastq
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /data/user/huangf/Arginini/script-test/Only-long-read/snakemake/.snakemake/log/2020-08-16T160645.278170.snakemake.log
